{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12161655,"sourceType":"datasetVersion","datasetId":7659475},{"sourceId":12161968,"sourceType":"datasetVersion","datasetId":7659702}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Load uploaded files\n# Paths updated to Kaggle input directory format\ndf_test = pd.read_csv('/kaggle/input/test-sample/train.csv')\ndf_val = pd.read_csv('/kaggle/input/test-sample/val.csv')\ndf_combined = pd.concat([df_test, df_val], ignore_index=True)\nsubmission_df = pd.read_csv('/kaggle/input/test-sample1/sample_submission.csv')\n\n# Drop unnecessary columns\ndf_combined = df_combined.drop(columns=['team_name', 'shortname', 'bike_name', 'circuit_name','rider_name'])\ndf_combined['Penalty'] = df_combined['Penalty'].fillna('None')\n\n# Normalize Unique ID and Rider Name columns\ndf_combined['Unique ID'] = df_combined['Unique ID'].astype(str).str.strip().str.upper()\n#df['rider_name'] = df['rider_name'].astype(str).str.strip().str.lower()\nsubmission_df['Unique ID'] = submission_df['Unique ID'].astype(str).str.strip().str.upper()\n#submission_df['rider_name'] = submission_df['rider_name'].astype(str).str.strip().str.lower()\n\n# Auto-detect the lap time column name if not explicitly available\npossible_lap_time_columns = [col for col in df_combined.columns if 'lap' in col.lower() and 'time' in col.lower()]\nif possible_lap_time_columns:\n    lap_time_column = possible_lap_time_columns[0]  # Choose the most relevant match\n    print(f\"Detected lap time column: {lap_time_column}\")\n\n    # Merge on rider_name to find matching lap times\n    merged_df = pd.merge(submission_df, df_combined[[ 'Unique ID', lap_time_column]] , how='left')\n\n    # Drop duplicates if any\n    merged_df = merged_df.drop_duplicates(subset=['Unique ID'])\n\n    # Select required columns\n    lap_times_from_val = merged_df[['Unique ID', lap_time_column]]\n\n    # Print the results\n    print(\"\\nLap times and Unique IDs from validation set:\")\n    print(lap_times_from_val.head())\n\n    # Save to CSV\n    lap_times_from_val.to_csv('solution.csv', index=False)\n    print(\"\\nSaved lap times with Unique IDs to 'solution.csv'\")\nelse:\n    print(\"‚ùå Could not detect a lap time column automatically. Please check your dataset.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:40:02.669480Z","iopub.execute_input":"2025-06-14T15:40:02.669847Z","iopub.status.idle":"2025-06-14T15:40:24.724829Z","shell.execute_reply.started":"2025-06-14T15:40:02.669820Z","shell.execute_reply":"2025-06-14T15:40:24.724009Z"}},"outputs":[{"name":"stdout","text":"Detected lap time column: Lap_Time_Seconds\n\nLap times and Unique IDs from validation set:\n  Unique ID  Lap_Time_Seconds\n0    288307            70.667\n1    704288           103.497\n2    951491            78.400\n3   2591721            81.338\n4   1202653           108.626\n\nSaved lap times with Unique IDs to 'solution.csv'\n","output_type":"stream"}],"execution_count":2}]}